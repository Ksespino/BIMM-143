---
title: "Class09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Hands On

```{r}
# Save your input data file to a new 'data' directory
fna.data <-  "WisconsinCancer.csv"
```

```{r}
# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data)
```
Lets see the first six rows of data 
```{r}
head(wisc.df)
```
```{r}
View(wisc.df)
```
How many samples (i.e. patients) are in this data set? 
```{r}
nrow(wisc.df)
```
Next use as.matrix() to convert the other features (i.e. columns) of the data (in columns 3 through 32) to a matrix. Store this in a variable called wisc.data.
```{r}
# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[,3:32])
```

Add rownames to our new matric of data

```{r}
row.names(wisc.data) <- wisc.df$id
```
Finally, setup a separate new vector called diagnosis to be 1 if a diagnosis is malignant ("M") and 0 otherwise. Note that R coerces TRUE to 1 and FALSE to 0.
```{r}
wisc.df$diagnosis
```

How many cancer (M) and non cancer samples do we have in our data set?
```{r}
 table(wisc.df$diagnosis)
```
```{r}
#diagnosis <- as.numeric(wisc.df$diagnosis)
#tmp <- rep(0, nrow(wisc.df))
diagnosis<- as.numeric(wisc.df$diagnosis == "M")
```
Hands on Questions
Q1. How many observations are in this dataset?
```{r}
nrow(wisc.data)
```

Q2. How many variables/features in the data are
suffixed with _mean?
```{r}
 grep("_mean", colnames(wisc.data))

```

Q3. How many of the observations have a malignant diagnosis?
```{r}
 sum (diagnosis)
```
#Section 2 

Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the colMeans() and apply() functions like youâ€™ve done before.
```{r}
# Check column means and standard deviations
 colMeans(wisc.data)

 apply(wisc.data,2,sd)
```

Now lets do PCA
```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale= TRUE)
summary(wisc.pr)
```

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
```{r}
looking at the table above 0.4427

```

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
```{r}
you would look at the proportion of variance above and realize you need to sum PC1-PC5 so 5 components
```

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
```{r}
seven 
```
We need to make our own PCA plot 
btw 1 is white and 2 is red so +1 gives red
```{r}
plot( wisc.pr$x[, 1], wisc.pr$x[,2])
plot(wisc.pr$x[, 1], wisc.pr$x[,2], col=diagnosis+1)
plot(wisc.pr$x[, 1], wisc.pr$x[,2], col=diagnosis+1,
     xlab= "PC1", ylab="PC2")
```
What if we wanted to plot PC1 vs PC3?
```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[,3], col=diagnosis+1,
     xlab= "PC1", ylab="PC3")
```



This info is in the $sdev component of our PCA result
```{r}
varance <- wisc.pr$sdev^2
# Variance explained by each principal component: pve
pve <- round ( (varance/ sum(varance)) *100,1 )
```
```{r}
plot(pve, type= "o")
```


Make the plot a bit fancy :-o

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 ) 


#axes=false turns of the axes so that you can reset them after in the following line of code
```

Section 3 Hands On
## Hierarchial clustering of cancer case data
For hierarchial clustering we need a few things
1. Distance matrix **dist()** function
2. The **hclust()** function 
3. Use the **cutree()** function to find cluster membership vector

```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
```
```{r}
round( apply(wisc.data,2, sd),1)
```
```{r}
round (apply(data.scaled, 2, sd), 1)
```

Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to data.dist.
```{r}
data.dist <- dist(data.scaled)
```
Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to wisc.hclust.
```{r}
wisc.hclust <- hclust(data.dist, method= "complete" )
wisc.hclust
```


```{r}
plot(wisc.hclust)
```

Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
Hide
```{r}
 wisc.pr.hclust <- plot(wisc.hclust)
h <- "height"
abline(h=19, col="red", lty=2)
```
Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.

Section 5
## Cluster in PCA space

we need
1. distance matrix
2. clustering function
3. cutree

This was our PCA resut of PC1 vs Pc2
```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[,2], col=diagnosis+1,
     xlab= "PC1", ylab="PC2")
```
```{r}
pc.dist <- dist(wisc.pr$x[,1:2])
pc.hclust <- hclust(pc.dist, method="ward.D2")
plot(pc.hclust)
```

```{r}
grps3 <- cutree(pc.hclust, k=3, h= 19)
grps3
```
```{r}
#how many in each cluster
table(grps3)
```
```{r}
table(grps3, diagnosis)
```
Section 7
```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```
The 2 blue points are 2 new patients, new data recorded as part of the new data set above
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis + 1)
points(npc[,1], npc[,2], col="blue", pch=15, cex=3)

```













